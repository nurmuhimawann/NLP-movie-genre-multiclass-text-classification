# -*- coding: utf-8 -*-
"""Submission_NLP_TensorFlow_Multiclass_Text_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1it7w2M3Q5nlnp9TKah0vkABeosoMu3kQ

# **Profile** <br>
*Nama: Nur Muhammad Himawan* <br>
*Domisili: Banyuwangi, Jawa Timur* <br>
*E-mail: muhammad.himawan73@gmail.com* <br>
*Path: Machine Learning & Front End Development* <br>
*Progam: Studi Independen Batch 3 - Kampus Merdeka*

## **Proyek: Membuat Model NLP dengan TensorFlow**

Selamat, Anda telah menyelesaikan modul Natural Language processing. Anda sudah mempelajari dasar-dasar machine learning dan bagaimana jaringan saraf bekerja pada NLP. Untuk bisa melanjutkan ke modul selanjutnya, Anda harus mengirimkan submission berupa proyek membuat model untuk klasifikasi teks.

**Dataset Preparation**
"""

# mount drive
from google.colab import drive
drive.mount('/content/drive')

# API my kaggle
! mkdir ~/.kaggle
! cp '/content/drive/MyDrive/Colab Notebooks/Kaggle API/kaggle.json' ~/.kaggle/kaggle.json
! chmod 600 ~/.kaggle/kaggle.json
! ls ~/.kaggle

# source dataset -> https://www.kaggle.com/datasets/lykin22/movie-genre-data
! kaggle datasets download lykin22/movie-genre-data

# unzip
! unzip movie-genre-data.zip -d /content/data/

"""**Load Dataset**

Large Movie Review Dataset: Straight from the boon of the Stanford AI Laboratory, this movie review dataset features 25,000 reviews of highly-polarizing films along with another 25,000 reviews specifically designed for training. 
"""

# read_csv
import pandas as pd
df = pd.read_csv('/content/data/kaggle_movie_train.csv')
df

# data info
df.info()

"""**Data Preprocessing**"""

# drop unused columns
df_new = df.drop(columns=['id'])
df_new

# check frequency distribution
df_new['genre'].value_counts()

"""use top 5 feature drama, thriller, comedy, action and sci-fi"""

# slice use .loc
columns = ['drama', 'thriller', 'comedy', 'action', 'sci-fi']
df_new = df_new.loc[df_new['genre'].isin(columns)]
df_new

# check frequency distribution
df_new['genre'].value_counts()

"""**One-Hot Encoding**"""

# one-hot encoding
category = pd.get_dummies(df_new['genre'])
df_encode = pd.concat([df_new, category], axis=1)
df_encode = df_encode.drop(columns='genre')
df_encode

"""**Data Cleansing**"""

# library
import re
import string
from string import punctuation

# helper function to clean texts
def processText(text):
    # Remove HTML special entities (e.g. &amp;)
    text = re.sub(r'\&\w*;', '', text)
    #Convert @username to AT_USER
    text = re.sub('@[^\s]+','',text)
    # Remove tickers
    text = re.sub(r'\$\w*', '', text)
    # To lowercase
    text = text.lower()
    # Remove hyperlinks
    text = re.sub(r'https?:\/\/.*\/\w*', '', text)
    # Remove hashtags
    text = re.sub(r'#\w*', '', text)
    # Remove Punctuation and split 's, 't, 've with a space for filter
    text = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', text)
    # Remove words with 2 or fewer letters
    text = re.sub(r'\b\w{1,2}\b', '', text)
    # Remove whitespace (including new line characters)
    text = re.sub(r'\s\s+', ' ', text)
    # Remove single space remaining at the front of the text.
    text = text.lstrip(' ') 
    # Remove characters beyond Basic Multilingual Plane (BMP or emoji) of Unicode:
    text = ''.join(c for c in text if c <= '\uFFFF')

    return text

# clean dataframe's text column
df_encode['text'] = df_encode['text'].apply(processText)
# preview cleaned text
df_encode[['text']]
df_clean = df_encode
df_clean

"""**Stemming**"""

# create stemmer
import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

ps = PorterStemmer()

# helper function to clean tweets
def stemmText(text):
  text = ps.stem(text)
  return text

# clean dataframe's text column
df_clean['text'] = df_clean['text'].apply(stemmText)
# preview data
df_clean[['text']]
df_final = df_clean
df_final

"""**Split Data**"""

# change value to numpy array
reviews = df_encode['text'].values 
label = df_encode[['action', 'comedy', 'drama', 'sci-fi', 'thriller']].values

# view text
reviews

# view labels
label

# split train test data
from sklearn.model_selection import train_test_split

text_train, text_test, label_train, label_test = train_test_split(reviews, label, test_size=0.2, random_state=42)

"""**Tokenization & Sequencing**"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#tokenizer
tokenizer = Tokenizer(num_words=50000, oov_token='-')
tokenizer.fit_on_texts(text_train)
tokenizer.fit_on_texts(text_test)

#sequences
sequence_train = tokenizer.texts_to_sequences(text_train)
sequence_test = tokenizer.texts_to_sequences(text_test)

#padding
padded_train = pad_sequences(sequence_train)
padded_test = pad_sequences(sequence_test)

"""**Building Model**"""

# function callbacks
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.90 and logs.get('val_accuracy')>0.90):
      print("\nAkurasi sudah mencapai 90%, Training Berhenti!")
      self.model.stop_training = True

callbacks = myCallback()

# sequential
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=50000, output_dim=64),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(5, activation='softmax')
])

model.summary()

# compile
model.compile(loss='categorical_crossentropy', 
              optimizer='Adam',
              metrics=['accuracy'],
              sample_weight_mode='temporal')

# modelling
history = model.fit(padded_train, label_train,
                    batch_size=50,
                    epochs=50,
                    validation_data=(padded_test, label_test),
                    verbose=2,
                    callbacks=[callbacks])

"""**Evaluation**"""

# model evaluation
import matplotlib.pyplot as plt

# plot of accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['accuracy', 'val_accuracy'], loc='lower right')
plt.show()

# plot of loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['loss', 'val_loss'], loc='upper right')
plt.show()

"""## **Kriteria Penilaian**

Berikut kriteria submission yang harus Anda penuhi:

- Dataset yang akan dipakai bebas, namun minimal memiliki 1000 sampel. **(done)**
- Harus menggunakan LSTM dalam arsitektur model. **(done)**
- Harus menggunakan model sequential. **(done)**
- Validation set sebesar 20% dari total dataset. **(done)**
- Harus menggunakan Embedding. **(done)**
- Harus menggunakan fungsi tokenizer. **(done)**
- Akurasi dari model minimal 75% pada train set dan validation set. **(done)**

Anda dapat menerapkan beberapa saran untuk mendapatkan nilai tinggi, berikut sarannya:

- Akurasi dari model di atas 80%. **(done)**

- Mengimplementasikan callback. **(done)**

- Membuat plot loss dan akurasi pada saat training dan validation. **(done)** <br>


Detail penilaian submission:

- Bintang 1 : Semua ketentuan terpenuhi, namun terdapat indikasi plagiat yaitu dengan menggunakan proyek orang lain dan hanya mengubah kontennya saja.

- Bintang 2 : Semua ketentuan terpenuhi, namun penulisan kode berantakan.

- Bintang 3 : Semua ketentuan terpenuhi namun hanya mengikuti seperti apa yang ada pada modul.

- Bintang 4 : Semua ketentuan terpenuhi, dataset memiliki minimal 2000 sampel data dan akurasi pada training set dan validation set di atas 85%.

- Bintang 5 : Semua ketentuan terpenuhi, dataset memiliki 3 kelas atau lebih dan minimal 2000 sampel data. Serta akurasi pada training set dan validation set di atas 90%. **(done)**

## **Ketentuan Berkas Submission**

Beberapa poin yang perlu diperhatikan ketika mengirimkan berkas submission:

- Menggunakan bahasa pemrograman Python. **(done)**

- Mengirimkan pekerjaan Anda dalam bentuk berkas ipynb dan py dalam 1 folder yang telah di zip. **(done)**

- File ipynb yang dikirim telah dijalankan terlebih dahulu sehingga output telah ada tanpa reviewer perlu menjalankan ulang notebook. **(done)**
"""